"""
Inference wrapper for the Muformer model. 
Example usage:
    python landscape.py \
        --pretrained-model="../ur50-pcomb-prot_pmlm_1b-3x16-ckpt-checkpoint_best.pt" \
        --muformer-model="../muformer-l-BLAT_ECOLX_Ranganathan2015_CFX.pt ../muformer-l-BLAT_ECOLX_Ranganathan2015_CFX.rank.wd0.1.dr0.5.all.pt ../muformer-l-BLAT_ECOLX_Ranganathan2015_CFX.rank.wd1.0.dr0.5.all.pt ../muformer-l-BLAT_ECOLX_Ranganathan2015_CFX.rank.wd10.0.dr0.5.all.pt ../muformer-l-BLAT_ECOLX_Ranganathan2015_CFX.spearman.wd0.1.dr0.5.all.pt ../muformer-l-ur90-BLAT_ECOLX_Ranganathan2015_CFX.rank.wd0.1.dr0.5.all.pt"
"""

import os
import torch
from pathlib import Path
import argparse
import numpy as np
from tqdm import tqdm
import pandas as pd
import random

import sys
sys.path.insert(0, os.path.join(Path(__file__).parent))
sys.path.append(os.path.join(Path(__file__).parent, "src/"))

from .landscape.muformer.muformer_landscape.src.model import Muformer
from .landscape.muformer.muformer_landscape.src.data import FairseqLMEncoder
from fairseq import checkpoint_utils
import flexs



def load_ckpt(args):
    tokendict_path = os.path.join(Path(__file__).parent, "src/protein/")
    arg_overrides = { 
        'data': tokendict_path
    }
    _models, _, task = checkpoint_utils.load_model_ensemble_and_task(args.pretrained_model.split(os.pathsep), 
                                                                arg_overrides=arg_overrides)
    encoder = _models[0]
    encoder_dim = encoder.args.encoder_embed_dim
    num_heads = encoder.args.encoder_attention_heads

    lm_encoder = FairseqLMEncoder(task)

    model = Muformer(encoder=encoder, encoder_dim=encoder_dim, hidden_size=args.hidden_size, num_heads=num_heads, dropout=args.dropout,
            decoder_name=args.decoder_name, freeze_lm=False, 
            joint_mode_enabled=args.joint_mode_enabled, joint_method=args.joint_method).to(args.device)
    
    # Zero-shot: comment out the following line
    pt = torch.load(args.muformer_model)
    model_dict = model.state_dict()
    model_pretrained_dict = {k: v for k, v in pt['model_state_dict'].items() if k in model_dict}
    model_dict.update(model_pretrained_dict)
    model.load_state_dict(model_dict)

    return model, lm_encoder


class MuformerLandscapeList():
    def __init__(self, args, starting_sequence):
        muformer_model_paths = args.muformer_model.strip().split(" ")
        self.muformer_models = []
        for i, muformer_model_path in enumerate(muformer_model_paths):
            args.muformer_model = muformer_model_path
            if i < 4:
                args.device = i
            else:
                args.device = (i + 1) % 4
                
            self.muformer_models.append(MuformerLandscape(args, starting_sequence))

        self.starting_sequence = starting_sequence

    def _mutation_to_sequence(self, mutation):
        '''
        Parameters
        ----------
        mutation: ';'.join(WiM) (wide-type W at position i mutated to M)
        '''
        sequence = self.starting_sequence
        for mut in mutation.split(';'):
            wt_aa = mut[0]
            mt_aa = mut[-1]
            pos = int(mut[1:-1])
            assert wt_aa == sequence[pos - 1],\
                    "%s: %s->%s (WT: %s)"%(pos, wt_aa, mt_aa, sequence[pos - 1])
            sequence = sequence[:(pos - 1)] + mt_aa + sequence[pos:]
        return sequence


    def predict_mutation(self, mutations, normalize=False):
        sequences = []
        for mutation in mutations:
            sequence = self._mutation_to_sequence(mutation)
            sequences.append(sequence)
        return self.get_fitness(sequences, normalize)


    def get_fitness(self, sequences, normalize=False):
        scores = []
        embeddings = []
        ensemble_uncertainties = []
        for seq in sequences:
            score, embedding, ensemble_uncertainty = self.predict(seq, normalize)
            scores.append(score)
            embeddings.append(embedding)
            ensemble_uncertainties.append(ensemble_uncertainty)
        return scores, embeddings, ensemble_uncertainties


    def predict(self, sequence, normalize=False):
        # sequential
        # for i, muformer_model in enumerate(self.muformer_models):
        #     if i == 0:
        #         score, embedding = muformer_model.predict(sequence)
        #         score = [score]
        #     else:
        #         score_, embedding_ = muformer_model.predict(sequence)
        #         score.append(score_)
        #         embedding = np.concatenate([embedding, embedding_])
        # # aggregate scores
        # avg_score = np.mean(score)
        # ensemble_uncertainty = np.std(score)
        # return avg_score, embedding, ensemble_uncertainty

        # parallel (threading)
        from concurrent.futures import ThreadPoolExecutor, as_completed, wait, ALL_COMPLETED
        embedding = None

        def predict_single_model(muformer_model, sequence, normalize):
            return muformer_model.predict(sequence, normalize)

        with ThreadPoolExecutor(max_workers=len(self.muformer_models)) as executor:
            for res in executor.map(predict_single_model, self.muformer_models, [sequence] * len(self.muformer_models), [normalize] * len(self.muformer_models)):
                if embedding is None:
                    score, embedding = res
                    score = [score]
                else:
                    score_, embedding_ = res
                    score.append(score_)
                    embedding = np.concatenate([embedding, embedding_])

        # aggregate scores
        avg_score = np.mean(score)
        ensemble_uncertainty = np.std(score)
        return avg_score, embedding, ensemble_uncertainty


class MuformerLandscape(flexs.Landscape):
    def __init__(self):
        name = "MuformerLandscape"
        starting_sequence = "MSIQHFRVALIPFFAAFCLPVFAHPETLVKVKDAEDQLGARVGYIELDLNSGKILESFRPEERFPMMSTFKVLLCGAVLSRVDAGQEQLGRRIHYSQNDLVEYSPVTEKHLTDGMTVRELCSAAITMSDNTAANLLLTTIGGPKELTAFLHNMGDHVTRLDRWEPELNEAIPNDERDTTMPAAMATTLRKLLTGELLTLASRQQLIDWMEADKVAGPLLRSALPAGWFIADKSGAGERGSRGIIAALGPDGKPSRIVVIYTTGSQATMDERNRQIAEIGASLIKHW"
        super().__init__(name)
        class Args:
            def __init__(self):
                self.decoder_name = 'mono'
                self.pretrained_model = 'src/flexs/flexs/landscapes/landscape/muformer/ur50-pcomb-prot_pmlm_1b-3x16-ckpt-checkpoint_best.pt'
                self.muformer_model = 'src/flexs/flexs/landscapes/landscape/muformer/muformer-l-BLAT_ECOLX_Ranganathan2015_CFX.pt'
                self.device = "cuda:0"

        args = Args()
        args.encoder_dim = getattr(args, 'encoder_dim', 1280)
        args.hidden_size = getattr(args, 'hidden_size', 256)
        args.num_heads = getattr(args, 'num_heads', 12)
        args.dropout = getattr(args, 'dropout', 0.1)
        args.joint_mode_enabled = getattr(args, 'joint_mode_enabled', True)
        args.joint_method = getattr(args, 'joint_method', 'average')

        self.device = getattr(args, 'device', 'cuda')
    self.muformer, self.lm_encoder = load_ckpt(args)
        self.muformer = self.muformer.to(self.device)
        print(f"Muformer model is loaded on the device: {self.device}")
        self.muformer.eval()

        self.starting_sequence = starting_sequence

        # for normalize
        print(f"Landscape initialization: Loading Muformer model from {args.muformer_model}")
        self.WT_fitness = self.get_fitness([self.starting_sequence], normalize=False)
        print(f"WT fitness: {self.WT_fitness}")
        esbl_file_path = "src/flexs/flexs/landscapes/landscape/muformer/muformer_landscape/esbl_sequences.csv"
        esbl_sequences = pd.read_csv(esbl_file_path)["sequence"]
        print(f"Number of ESBL sequences: {len(esbl_sequences)}")
        ESBL_fitness = self.get_fitness(esbl_sequences, normalize=False)
        self.ESBL_fitness_median = np.median(ESBL_fitness)
        self.ESBL_fitness_std = np.std(ESBL_fitness)
        print(f"Median of ESBL fitness: {self.ESBL_fitness_median}")
        print(f"Std of ESBL fitness: {self.ESBL_fitness_std}")
        print("Landscape initialization: Done")

    def _mutation_to_sequence(self, mutation):
        '''
        Parameters
        ----------
        mutation: ';'.join(WiM) (wide-type W at position i mutated to M)
        '''
        sequence = self.starting_sequence
        if mutation == "":
            return sequence
        for mut in mutation.split(';'):
            wt_aa = mut[0]
            mt_aa = mut[-1]
            pos = int(mut[1:-1])
            assert wt_aa == sequence[pos - 1],\
                    "%s: %s->%s (WT: %s)"%(pos, wt_aa, mt_aa, sequence[pos - 1])
            sequence = sequence[:(pos - 1)] + mt_aa + sequence[pos:]
        return sequence


    def predict_mutation(self, mutations, normalize=False):
        sequences = []
        for mutation in mutations:
            sequence = self._mutation_to_sequence(mutation)
            sequences.append(sequence)
        return self.get_fitness(sequences, normalize)
    
    def _fitness_function(self, sequences):
        scores = []
        for seq in sequences:
            score, embedding = self.predict(seq, normalize=False)
            scores.append(score)
        # self.cost += len(sequences)
        return np.array(scores)

    def get_fitness(self, sequences, normalize=False):
        scores = []
        embeddings = []
        for seq in sequences:
            score, embedding = self.predict(seq, normalize)
            scores.append(score)
            embeddings.append(embedding)
        self.cost += len(sequences)
        return scores


    def predict(self, sequence, normalize=False):        
        with torch.no_grad():
            feat1 = self.lm_encoder.encode([sequence, self.starting_sequence])
            # feat2 = self.lm_encoder.encode([native_sequence, sequence])

            glob_feat1 = feat1['glob_feat'].to(self.device)
            # glob_feat2 = feat2['glob_feat'].to(self.device)

            glob_mask1 = feat1['glob_mask'].to(self.device)
            # glob_mask2 = feat2['glob_mask'].to(self.device)
            score, embedding = self.muformer(x1=glob_feat1, x2=None, x1_mask=glob_mask1, include_embedding=True)
            score = score.view(-1).cpu().numpy()[0]
            if normalize:
                score = (score - self.WT_fitness) / (self.ESBL_fitness_median - self.WT_fitness)
            embedding = embedding.cpu().numpy()
            assert len(embedding.shape) == 1
            return score, embedding
    
    def gradient_function(self, sequences):
        feat1 = self.lm_encoder.encode(sequences)
        print(f"feat1: {feat1}")
        glob_feat1 = feat1['glob_feat'].to(self.device)
        glob_mask1 = feat1['glob_mask'].to(self.device)
        score, embedding = self.muformer(x1=glob_feat1, x2=None, x1_mask=glob_mask1, include_embedding=True)
        score = score.view(-1).cpu().numpy()



if __name__ == "__main__":
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--pretrained-model', action='store', help='Path to the pretrained LM model')
    parser.add_argument('--muformer-model', action='store', help='Path to the finetuned Muformer model')
    parser.add_argument('--device', action='store', default='cuda:0', help='Device used')
    args = parser.parse_args()

    muformer_landscape = MuformerLandscape()
    muformer_landscape.gradient_function(["MSIQHFRVALIPFFAAFCLPVFAHPETLVKVKDAEDQLGARVGYIELDLNSGKILESFRPEERFPMMSTFKVLLCGAVLSRVDAGQEQLGRRIHYSQNDLVEYSPVTEKHLTDGMTVRELCSAAITMSDNTAANLLLTTIGGPKELTAFLHNMGDHVTRLDRWEPELNEAIPNDERDTTMPAAMATTLRKLLTGELLTLASRQQLIDWMEADKVAGPLLRSALPAGWFIADKSGAGERGSRGIIAALGPDGKPSRIVVIYTTGSQATMDERNRQIAEIGASLIKHW"])

    # TEM-1 beta-lactamase
    landscape = MuformerLandscapeList(args, "MSIQHFRVALIPFFAAFCLPVFAHPETLVKVKDAEDQLGARVGYIELDLNSGKILESFRPEERFPMMSTFKVLLCGAVLSRVDAGQEQLGRRIHYSQNDLVEYSPVTEKHLTDGMTVRELCSAAITMSDNTAANLLLTTIGGPKELTAFLHNMGDHVTRLDRWEPELNEAIPNDERDTTMPAAMATTLRKLLTGELLTLASRQQLIDWMEADKVAGPLLRSALPAGWFIADKSGAGERGSRGIIAALGPDGKPSRIVVIYTTGSQATMDERNRQIAEIGASLIKHW")
    # # 1. Parse the dataset for fine-tuning.
    # df = pd.read_csv('../mmc4-cfx.aligned.tsv', sep='\t')
    # # df = pd.read_csv('../CFX_test_from_pan.tsv', sep='\t')
    # mutations = df['mutation'].tolist()
    # scores = df['score'].tolist()

    # # 2. Get the fitness of the dataset.
    # uncertainties = [] 
    # delta_scores = []
    # pred_scores = []
    # max_pred_value = -np.inf
    # max_pred_mutant = None
    # min_pred_value = np.inf
    # max_label_value = -np.inf
    # max_label_mutant = None
    # min_label_value = np.inf
    # dataset_size = 0
    # for mutation, score in tqdm(zip(mutations, scores)):
    #     fitness, _, uncertainty = landscape.predict_mutation(mutation)
    #     uncertainties.append(uncertainty)
    #     pred_scores.append(fitness)
    #     delta_scores.append(abs(fitness - score))
    #     if score > max_label_value:
    #         max_label_mutant = mutation
    #     if fitness > max_pred_value:
    #         max_pred_mutant = mutation
    #     max_pred_value = max(max_pred_value, fitness)
    #     max_label_value = max(max_label_value, score)
    #     min_pred_value = min(min_pred_value, fitness)
    #     min_label_value = min(min_label_value, score)
    #     dataset_size += 1

    # # compute the spearman correlation between uncertainties and delta_scores
    # import scipy.stats


    # 3. Analyze some specific mutations.
    # muts = ['L79R;D113S;D129T;G236A', 'E56H;I93H;H110W;L137P;G236N', 'C121S;R162H;R238P;S254P']  # predict_mutation
    # for mut in muts:
    #     fitness, embedding, uncertainty = landscape.predict_mutation(mut)
    #     print(f'[Mutation] {mut} \t= {fitness}')

    # 4. Save the embedding
    # file_path = "horizon_3_high_score_mutants.txt"
    # results = []
    # from tqdm import tqdm
    # with open(file_path, "r") as f:
    #     for line in tqdm(f):
    #         mutant, score = line.strip().split(" ")
    #         score = float(score)
    #         pred, embedding = landscape.predict_mutation(mutant)
    #     results.append([mutant, score, embedding])
    # # save results into npy file
    # np.save("horizon_3_high_score_mutants.npy", results)

    # RUN
    # starting_sequence = "MSIQHFRVALIPFFAAFCLPVFAHPETLVKVKDAEDQLGARVGYIELDLNSGKILESFRPEERFPMMSTFKVLLCGAVLSRVDAGQEQLGRRIHYSQNDLVEYSPVTEKHLTDGMTVRELCSAAITMSDNTAANLLLTTIGGPKELTAFLHNMGDHVTRLDRWEPELNEAIPNDERDTTMPAAMATTLRKLLTGELLTLASRQQLIDWMEADKVAGPLLRSALPAGWFIADKSGAGERGSRGIIAALGPDGKPSRIVVIYTTGSQATMDERNRQIAEIGASLIKHW"
    # fitness = landscape.get_fitness([starting_sequence], normalize=False)[0][0]    

    # 3CL protease, demo
    # protein_dict = {
    #     "3CL": "SGFRKMAFPSGKVEGCMVQVTCGTTTLNGLWLDDVVYCPRHVICTSEDMLNPNYEDLLIRKSNHNFLVQAGNVQLRVIGHSMQNCVLKLKVDTANPKTPKYKFVRIQPGQTFSVLACYNGSPSGVYQCAMRPNFTIKGSFLNGSCGSVGFNIDYDCVSFCYMHHMELPTGVHAGTDLEGNFYGPFVDRQTAQAAGTDTTITVNVLAWLYAAVINGDRWFLNRFTTTLNDFNLVAMKYNYEPLTQDHVDILGPLSAQTGIAVLDMCASLKELLQNGMNGRTILGSALLEDEFTPFDVVRQCSGVTFQ",
    #     "PLPro": "EVRTIKVFTTVDNINLHTQVVDMSMTYGQQFGPTYLDGADVTKIKPHNSHEGKTFYVLPNDDTLRVEAFEYYHTTDPSFLGRYMSALNHTKKWKYPQVNGLTSIKWADNNCYLATALLTLQQIELKFNPPALQDAYYRARAGEAANFCALILAYCNKTVGELGDVRETMSYLFQHANLDSCKRVLNVVCKTCGQQQTTLKGVEAVMYMGTLSYEQFKKGVQIPCTCGKQATKYLVQQESPFVMMSAPPAQYELKHGTFTCASEYTGNYQCGHYKHITSKETLYCIDGALLTKSSEYKGPITDVFYKENSYTTTIKAA",
    #     "RDRP": "SADAQSFLNRVCGVSAARLTPCGTGTSTDVVYRAFDIYNDKVAGFAKFLKTNCCRFQEKDEDDNLIDSYFVVKRHTFSNYQHEETIYNLLKDCPAVAKHDFFKFRIDGDMVPHISRQRLTKYTMADLVYALRHFDEGNCDTLKEILVTYNCCDDDYFNKKDWYDFVENPDILRVYANLGERVRQALLKTVQFCDAMRNAGIVGVLTLDNQDLNGNWYDFGDFIQTTPGSGVPVVDSYYSLLMPILTLTRALTAESHVDTDLTKPYIKWDLLKYDFTEERLKLFDRYFKYWDQTYHPNCVNCLDDRCILHCANFNVLFSTVFPPTSFGPLVRKIFVDGVPFVVSTGYHFRELGVVHNQDVNLHSSRLSFKELLVYAADPAMHAASGNLLLDKRTTCFSVAALTNNVAFQTVKPGNFNKDFYDFAVSKGFFKEGSSVELKHFFFAQDGNAAISDYDYYRYNLPTMCDIRQLLFVVEVVDKYFDCYDGGCINANQVIVNNLDKSAGFPFNKWGKARLYYDSMSYEDQDALFAYTKRNVIPTITQMNLKYAISAKNRARTVAGVSICSTMTNRQFHQKLLKSIAATRGATVVIGTSKFYGGWHNMLKTVYSDVENPHLMGWDYPKCDRAMPNMLRIMASLVLARKHTTCCSLSHRFYRLANECAQVLSEMVMCGGSLYVKPGGTSSGDATTAYANSVFNICQAVTANVNALLSTDGNKIADKYVRNLQHRLYECLYRNRDVDTDFVNEFYAYLRKHFSMMILSDDAVVCFNSTYASQGLVASIKNFKSVLYYQNNVFMSEAKCWTETDLTKGPHEFCSQHTMLVKQGDDYVYLPYPDPSRILGAGCFVDDIVKTDGTLMIERFVSLAIDAYPLTKHPNQEYADVFHLYLQYIRKLHDELTGHMLDMYSVMLTNDNTSRYWEPEFYEAMYTPHTVLQHHHHHHHHHH",
    #     "6M0J": "RVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFHHHHHH",
    # }

    # for protein_name, wt_sequence in protein_dict.items():
    #     print(f"Protein: {protein_name}")
    #     wt_fitness = landscape.get_fitness([wt_sequence], normalize=False)[0][0]
    #     wt_sequence_len = len(wt_sequence)
    #     print(f'[Wild type sequence] {wt_sequence} \t= {wt_fitness}')  # => [-1.0017444， -0.93]
    #     print(f"Wild type sequence length: {wt_sequence_len}")

    #     vocab = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']

    #     random_mutation_path = f"random_mutations_{protein_name}_1017.txt"
    #     random_mutations = []
    #     num_higher = 0

    #     with open(random_mutation_path, "w") as f:
    #         random_mutants_num = 50000
    #         for i in tqdm(range(random_mutants_num)):
    #             if random.random() < 0.5:
    #                 # single point mutation
    #                 pos = random.randint(0, wt_sequence_len - 1)
    #                 wt_aa = wt_sequence[pos]
    #                 mt_aa = random.choice(vocab)
    #                 mutant_sequence = wt_sequence[:pos] + mt_aa + wt_sequence[pos + 1:]
    #                 fitness_score = landscape.get_fitness([mutant_sequence], normalize=False)[0][0]
    #                 is_higher_than_wt = fitness_score > wt_fitness
    #                 random_mutations.append([f"{wt_aa}{pos + 1}{mt_aa}", fitness_score, is_higher_than_wt])
    #                 if is_higher_than_wt:
    #                     num_higher += 1
    #             else:
    #                 # double points mutation
    #                 pos1 = random.randint(0, wt_sequence_len - 1)
    #                 pos2 = random.randint(0, wt_sequence_len - 1)
    #                 while pos1 == pos2:
    #                     pos2 = random.randint(0, wt_sequence_len - 1)
    #                 wt_aa1 = wt_sequence[pos1]
    #                 wt_aa2 = wt_sequence[pos2]
    #                 mt_aa1 = random.choice(vocab)
    #                 mt_aa2 = random.choice(vocab)
    #                 mutant_sequence = wt_sequence[:pos1] + mt_aa1 + wt_sequence[pos1 + 1:]
    #                 mutant_sequence = mutant_sequence[:pos2] + mt_aa2 + mutant_sequence[pos2 + 1:]
    #                 fitness_score = landscape.get_fitness([mutant_sequence], normalize=False)[0][0]
    #                 is_higher_than_wt = fitness_score > wt_fitness
    #                 random_mutations.append([f"{wt_aa1}{pos1 + 1}{mt_aa1};{wt_aa2}{pos2 + 1}{mt_aa2}", fitness_score, is_higher_than_wt])
    #                 if is_higher_than_wt:
    #                     num_higher += 1

    #         # sorted by fitness score
    #         random_mutations = sorted(random_mutations, key=lambda x: x[1], reverse=True)
    #         for random_mutation in random_mutations:
    #             f.write(f"{random_mutation[0]} {random_mutation[1]} {random_mutation[2]}\n")

    #         print(f"num_higher: {num_higher} / {random_mutants_num}")
            
    # WT_avg_fitness = landscape.get_fitness([starting_sequence])[0][0]
    # esbl_file_path = "/home/guoqingliu/DARWIN/RL/landscape/muformer/muformer_landscape/esbl_sequences.csv"
    # esbl_sequences = pd.read_csv("esbl_sequences.csv")["sequence"]
    # ESBL_fitness = landscape.get_fitness(esbl_sequences)[0]
    # ESBL_fitness_max = np.max(ESBL_fitness)
    # ESBL_fitness_min = np.min(ESBL_fitness)
    # ESBL_fitness_median = np.median(ESBL_fitness)

    # From xxx.txt to xxx.json (to include embedding)
    # file_path = "./horizon_5_ensemble3_1007_all_explored_sequences.txt"
    # sorted_results = []
    # with open(file_path, "r") as f:
    #     for i, line in enumerate(f):
    #         print(f"{i}/697497")
    #         mutant_and_score = line.strip().split(" ")
    #         if len(mutant_and_score) == 1:
    #             continue
    #         mutant, score = mutant_and_score
    #         score = float(score)
    #         pred, embedding, _ = landscape.predict_mutation(mutant)
    #         # print("pred, score", pred, score)
    #         assert abs(score - pred) <= 0.01, [mutant, score, pred] 
    #         sorted_results.append([mutant, score, embedding])
    #         # if i >= 100:
    #             # break
    
    # df = pd.DataFrame(sorted_results, columns=['mutation', 'score', 'embedding'])
    # df.to_json(file_path.replace("txt", "json"), orient='records', lines=True)

    # 12/25/2023 Get all possible combinatorial evolutionary paths according to four single-point mutations: A40G,E102K,M180T,G236S.
    # import itertools
    # import numpy as np

    # def list_permutations(lst):
    #     return list(itertools.permutations(lst))
    
    # mutations = ['A40G','E102K','M180T','G236S']
    # possible_evolutionary_paths = [";".join(path) for path in list_permutations(mutations)]
    # print("Number of all possible evolutionary paths: ", len(possible_evolutionary_paths))  # => 24

    # GT_evolutionary_paths = ['G236S;E102K;A40G;M180T', 'E102K;G236S;A40G;M180T']

    # def get_fitness_of_evolutionary_path(landscape, evolutionary_path):
    #     fitnesses = []
    #     evolutionary_path_list = evolutionary_path.split(";")
    #     path_len = len(evolutionary_path_list)
    #     for i in range(path_len):
    #         evolutionary_sub_path = ";".join(evolutionary_path_list[:i + 1])
    #         fitness, _, _ = landscape.predict_mutation(evolutionary_sub_path, normalize=True)
    #         fitnesses.append(fitness)
    #     return np.mean(fitnesses)
    
    # fitness_of_possible_evolutionary_paths = {}
    # for evolutionary_path in possible_evolutionary_paths:
    #     fitness = get_fitness_of_evolutionary_path(landscape, evolutionary_path)
    #     fitness_of_possible_evolutionary_paths[evolutionary_path] = fitness

    # # rank the elements in the dictionary by value in descending order
    # sorted_fitness_of_possible_evolutionary_paths = sorted(fitness_of_possible_evolutionary_paths.items(), key=lambda x: x[1], reverse=True)
    # print(sorted_fitness_of_possible_evolutionary_paths)

    # print("average fitness of all possible evolutionary paths:", np.mean(list(fitness_of_possible_evolutionary_paths.values())))

    # fitness_of_GT_evolutionary_paths = {}
    # for evolutionary_path in GT_evolutionary_paths:
    #     fitness = get_fitness_of_evolutionary_path(landscape, evolutionary_path)
    #     fitness_of_GT_evolutionary_paths[evolutionary_path] = fitness

    # print("average fitness of all GT evolutionary paths (two paths):", np.mean(list(fitness_of_GT_evolutionary_paths.values())))
    # print(fitness_of_GT_evolutionary_paths)

    # 1/10/2024 
    protein_dict = {
        'TEM': 'MSIQHFRVALIPFFAAFCLPVFAHPETLVKVKDAEDQLGARVGYIELDLNSGKILESFRPEERFPMMSTFKVLLCGAVLSRVDAGQEQLGRRIHYSQNDLVEYSPVTEKHLTDGMTVRELCSAAITMSDNTAANLLLTTIGGPKELTAFLHNMGDHVTRLDRWEPELNEAIPNDERDTTMPAAMATTLRKLLTGELLTLASRQQLIDWMEADKVAGPLLRSALPAGWFIADKSGAGERGSRGIIAALGPDGKPSRIVVIYTTGSQATMDERNRQIAEIGASLIKHW',
    }

    for protein_name, wt_sequence in protein_dict.items():
        print(f"Protein: {protein_name}")
        wt_fitness = landscape.get_fitness([wt_sequence], normalize=False)[0][0]
        wt_sequence_len = len(wt_sequence)
        print(f'[Wild type sequence] {wt_sequence} \t= {wt_fitness}')  # => [-1.0017444， -0.93]
        print(f"Wild type sequence length: {wt_sequence_len}")

    esbl_file_path = "/home/v-zhaozhiyu/blob/guoqing/code/0906/DARWIN/RL/landscape/muformer/muformer_landscape/esbl_sequences.csv"
    esbl_sequences = pd.read_csv(esbl_file_path)["sequence"]
    print(f"Number of ESBL sequences: {len(esbl_sequences)}")
    ESBL_fitness = landscape.get_fitness(esbl_sequences, normalize=False)[0]
    ESBL_fitness_max = np.max(ESBL_fitness)
    ESBL_fitness_median = np.median(ESBL_fitness)
    ESBL_fitness_std = np.std(ESBL_fitness)
    print(f"Max of ESBL fitness: {ESBL_fitness_max}")
    print(f"Median of ESBL fitness: {ESBL_fitness_median}")
    print(f"Std of ESBL fitness: {ESBL_fitness_std}")

    # vocab = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X']
    vocab = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']
    # Muformer vocab
    # {'<s>': 0, '<pad>': 1, '</s>': 2, '<unk>': 3, 'A': 4, 'R': 5, 'N': 6, 'D': 7, 'C': 8, 'Q': 9, 'E': 10, 'G': 11, 'H': 12, 'I': 13, 'L': 14, 'K': 15, 'M': 16, 'F': 17, 'P': 18, 'S': 19, 'T': 20, 'W': 21, 'Y': 22, 'V': 23, 'X': 24, '-': 25, '<mask>': 26}

    START_POINT_NUM = 2
    END_POINT_NUM = 4
    total_random_mutations = []
    total_random_mutations.append([0, "", wt_fitness, False])  # add WT
    total_random_mutation_path = f"{protein_name}_{START_POINT_NUM}-{END_POINT_NUM}_points_random_mutations_20240131_test.csv"

    for point_num in range(START_POINT_NUM, END_POINT_NUM):
        print(f"Randomly generating {point_num}-point mutations")
        random_mutations = []
        num_higher = 0

        random_mutants_num = 50
        for _ in tqdm(range(random_mutants_num)):
            randomly_chosen_positions = random.sample(range(wt_sequence_len), point_num)
            for i in range(1, point_num):
                while randomly_chosen_positions[i] in randomly_chosen_positions[:i]:
                    print("rechoose")
                    randomly_chosen_positions[i] = random.randint(0, wt_sequence_len - 1)
            randomly_chosen_positions = sorted(randomly_chosen_positions)

            wt_aa_list = []
            mt_aa_list = []
            for pos in randomly_chosen_positions:
                wt_aa_list.append(wt_sequence[pos])
                mt_aa_list.append(random.choice(vocab))

            mutant_sequence = wt_sequence
            for i in range(point_num):
                mutant_sequence = mutant_sequence[:randomly_chosen_positions[i]] + mt_aa_list[i] + mutant_sequence[randomly_chosen_positions[i] + 1:]
            fitness_score = landscape.get_fitness([mutant_sequence], normalize=False)[0][0]
            is_higher_than_wt = fitness_score > wt_fitness
            if is_higher_than_wt:
                num_higher += 1
            random_mutations.append([point_num, f"{';'.join([wt_aa_list[i] + str(randomly_chosen_positions[i] + 1) + mt_aa_list[i] for i in range(point_num)])}", fitness_score, is_higher_than_wt])


        # Sorted by fitness score.
        random_mutations = sorted(random_mutations, key=lambda x: x[2], reverse=True)                
        print(f"num_higher: {num_higher} / {random_mutants_num}")

        total_random_mutations.extend(random_mutations)

    with open(total_random_mutation_path, "w") as f:
        import csv
        writer = csv.writer(f)
        writer.writerow(["point_num", "mutation", "fitness_score", "is_higher_than_wt"])
        for random_mutation in total_random_mutations:
            writer.writerow(random_mutation)

    ###################################################################################################################################################
    # 1/21/2024 post-processing the generated mutations via RL and different muformer models.
    # def read_topk_lines(file_list, k: int = 1000):
    #     """
    #     Read the top k lines of each file, each line is like "G236S;E277A 1.0469999313354492", obtain a list of mutations.
    #     """
    #     mutation_set = set()
    #     for file in file_list:
    #         with open(file, 'r') as f:
    #             for i in range(k+1):
    #                 line = f.readline()
    #                 if i != 0:
    #                     mutation_set.add(line.split()[0])
    #     return mutation_set

    # file_list = ["../../../horizon_3_ensemble1_model1_20240120_filtered_sequences.txt",
    #                 "../../../horizon_3_ensemble1_model2_20240118_filtered_sequences.txt",
    #                 "../../../horizon_3_ensemble1_model3_20240120_filtered_sequences.txt",
    #                 "../../../horizon_3_ensemble1_model4_20240118_filtered_sequences.txt",
    #                 "../../../horizon_3_ensemble1_model5_20240118_filtered_sequences.txt",
    #                 "../../../horizon_3_ensemble1_model6_20240120_filtered_sequences.txt"]
    # mutation_set_from_RL = read_topk_lines(file_list, k=200)
    # print("mutation_set_from_RL: ", len(mutation_set_from_RL))  # when k=1000, 5590 mutations

    # def seq2mutant(seq):
    #     """
    #     Given a sequence, return a list of mutations.
    #     """
    #     wild_type = "MSIQHFRVALIPFFAAFCLPVFAHPETLVKVKDAEDQLGARVGYIELDLNSGKILESFRPEERFPMMSTFKVLLCGAVLSRVDAGQEQLGRRIHYSQNDLVEYSPVTEKHLTDGMTVRELCSAAITMSDNTAANLLLTTIGGPKELTAFLHNMGDHVTRLDRWEPELNEAIPNDERDTTMPAAMATTLRKLLTGELLTLASRQQLIDWMEADKVAGPLLRSALPAGWFIADKSGAGERGSRGIIAALGPDGKPSRIVVIYTTGSQATMDERNRQIAEIGASLIKHW"

    #     if len(seq) != len(wild_type):
    #         return None
        
    #     mutant = []
    #     for i in range(len(seq)):
    #         if seq[i] != wild_type[i]:
    #             mutant.append(wild_type[i] + str(i+1) + seq[i])
    #     return ";".join(mutant)

    # esbl_file_path = "/home/guoqingliu/0906/DARWIN/RL/landscape/muformer/muformer_landscape/esbl_sequences.csv"
    # esbl_sequences = pd.read_csv(esbl_file_path)["sequence"]
    # print(len(esbl_sequences))
    # mutation_set_from_ESBL = set()
    # for seq in esbl_sequences:
    #     if seq2mutant(seq) != None:
    #         mutation_set_from_ESBL.add(seq2mutant(seq))
    # print("original esbl mutants number: ", len(mutation_set_from_ESBL))

    # total_mutant_list = list(mutation_set_from_RL.union(mutation_set_from_ESBL)) + [""]
    # print("total mutants number: ", len(total_mutant_list))

    # summary_dict = {}
    # # For validating the effectiveness of the proposed rank algorithm, we use valid/test dataset.
    # # valid_data_path = "/home/guoqingliu/0906/DARWIN/RL/landscape/muformer/muformer_landscape/CFX_test_from_pan_above_median.tsv"
    # # read the tsv file using pandas, and save the mutation column into a list, and save the score column into a list.
    # # df = pd.read_csv(valid_data_path, sep='\t')
    # # mutations = df['mutation'].tolist()

    # # If the mutation is WT, then replace it with "".
    # # for i in range(len(mutations)):
    #     # if mutations[i] == "WT":
    #         # mutations[i] = ""

    # # scores = df['score'].tolist()
    # # print("valid dataset size: ", len(mutations))  # 5688
    # # print(mutations)
    # # print(scores)
    # # print(type(scores[0]))
    # # summary_dict["gt"] = scores
    # # total_mutant_list = mutations

    # summary_dict["mutants"] = total_mutant_list
    # summary_dict["is_esbl"] = [ 1 if mutant in mutation_set_from_ESBL else 0 for mutant in total_mutant_list]
    # print("is esbl number: ", summary_dict["is_esbl"].count(1))  # 101
    # print("is not esbl number: ", summary_dict["is_esbl"].count(0))  # 5587

    # muformer_paths = args.muformer_model.strip().split(" ")
    # from pathlib import Path

    # for model_id, muformer_path in enumerate(muformer_paths):
    #     print(f"Muformer checkpoint {model_id}: {muformer_path}")
    #     args.muformer_model = muformer_path
    #     landscape = MuformerLandscape(args, "MSIQHFRVALIPFFAAFCLPVFAHPETLVKVKDAEDQLGARVGYIELDLNSGKILESFRPEERFPMMSTFKVLLCGAVLSRVDAGQEQLGRRIHYSQNDLVEYSPVTEKHLTDGMTVRELCSAAITMSDNTAANLLLTTIGGPKELTAFLHNMGDHVTRLDRWEPELNEAIPNDERDTTMPAAMATTLRKLLTGELLTLASRQQLIDWMEADKVAGPLLRSALPAGWFIADKSGAGERGSRGIIAALGPDGKPSRIVVIYTTGSQATMDERNRQIAEIGASLIKHW")
    #     fitnesses = landscape.predict_mutation(total_mutant_list, normalize=True)[0]
    #     landscape.muformer.to("cpu")

    #     summary_dict[Path(muformer_path).name] = fitnesses

    #     # get the rank of each mutant
    #     # [1,3,4,2] => [4,2,1,3]
    #     import scipy.stats as ss
    #     rank = ss.rankdata(-np.array(fitnesses))
    #     summary_dict[Path(muformer_path).name + "_rank"] = rank

    #     # compute the spearman correlation between fitnesses and rank:
    #     # assert ss.spearmanr(fitnesses, rank)[0] == -1.0, ss.spearmanr(fitnesses, rank)[0]
    
    # # get the average rank of each mutant
    # average_rank_list = []
    # std_rank_list = []
    # for i in range(len(total_mutant_list)):
    #     average_rank_list.append(np.mean([summary_dict[Path(muformer_path).name + "_rank"][i] for muformer_path in muformer_paths]))
    #     std_rank_list.append(np.std([summary_dict[Path(muformer_path).name + "_rank"][i] for muformer_path in muformer_paths]))
    # summary_dict["average_rank"] = average_rank_list
    # summary_dict["std_rank"] = std_rank_list
    # import scipy.stats as ss
    # summary_dict["calibrated_rank"] = ss.rankdata(np.array(average_rank_list))  # Important! Criteria for ranking!

    # # For validating the effectiveness of the proposed rank algorithm, we use valid/test dataset.
    # # assert len(summary_dict["gt"]) == len(summary_dict["average_rank"])
    # # reverse_average_rank = [-rank for rank in summary_dict["average_rank"]]
    # # print("spearmanr between GT and average rank algo: ", ss.spearmanr(summary_dict["gt"], reverse_average_rank)[0])

    # # rerank the indexes
    # idxs = np.argsort(summary_dict["calibrated_rank"])
    # summary_dict["mutants"] = [summary_dict["mutants"][idx] for idx in idxs]
    # summary_dict["is_esbl"] = [summary_dict["is_esbl"][idx] for idx in idxs]
    # for muformer_path in muformer_paths:
    #     summary_dict[Path(muformer_path).name] = [summary_dict[Path(muformer_path).name][idx] for idx in idxs]
    #     summary_dict[Path(muformer_path).name + "_rank"] = [summary_dict[Path(muformer_path).name + "_rank"][idx] for idx in idxs]
    # summary_dict["average_rank"] = [summary_dict["average_rank"][idx] for idx in idxs]
    # summary_dict["std_rank"] = [summary_dict["std_rank"][idx] for idx in idxs]
    # summary_dict["calibrated_rank"] = [summary_dict["calibrated_rank"][idx] for idx in idxs]

    # # summary_dict["gt"] = [summary_dict["gt"][idx] for idx in idxs]

    # # save the summary_dict into csv file using pandas
    # df = pd.DataFrame(summary_dict)
    # df.to_csv("summary_dict.csv", index=False)

